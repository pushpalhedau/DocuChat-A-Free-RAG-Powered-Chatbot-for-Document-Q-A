# -*- coding: utf-8 -*-
"""RAG LLM Chatbot Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YP6y5vVDSiiMQV16Nz8-fUx_4zmHmvVu
"""

# Install necessary libraries
!pip install langchain faiss-cpu sentence-transformers transformers gradio
!pip install -U langchain-community
!pip install pypdf # Install pypdf for PyPDFLoader

# Import necessary modules
from google.colab import files
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from IPython import get_ipython
from IPython.display import display

# Upload the PDF file
uploaded = files.upload()

# Load the uploaded file (replace filename if different)
loader = PyPDFLoader("ENTER THE UPLOADED FILE NAME")
documents = loader.load()

# Split into chunks (500 characters with 50 overlap)
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# Preview a chunk
# Note: Depending on the content and splitting, the first chunk might be different
# or empty if the first page has no text or is an image.
if docs:
    print(docs[10].page_content)
else:
    print("No text content found in the document after splitting.")

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Load the embedding model (free and fast)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create the FAISS vector store from the chunks
db = FAISS.from_documents(docs, embedding_model)

# Save to disk (optional)
db.save_local("faiss_index")

from transformers import pipeline

# Load a lightweight, fast FLAN-T5 model
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base")

def ask(question):
    # Step 1: Retrieve relevant documents
    relevant_docs = db.similarity_search(question, k=3)
    context = " ".join([doc.page_content for doc in relevant_docs])

    # Step 2: Create the prompt for the LLM
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"

    # Step 3: Generate answer using FLAN-T5
    response = qa_pipeline(prompt, max_length=500, do_sample=False)
    return response[0]['generated_text']

# Test it (remove comment from below line and enter question)
#print(ask(""))

!pip install gradio

import gradio as gr

def chatbot_interface(user_input):
    return ask(user_input)

gr.Interface(fn=chatbot_interface,
             inputs="text",
             outputs="text",
             title="Free RAG+LLM Chatbot",
             description="Ask questions about your uploaded document"
            ).launch()
